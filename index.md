## Select Projects

---

### PYPI package exploretransform

  + Exploretransform consists of a set of functions that accelerate exploratory data analysis. The functions return data in common Python data formats enabling practitioners to easily utilize or extend the outputs as part of their workflows.
    - [PYPI Page](https://pypi.org/project/exploretransform/)
    - [GitHub Repository](https://github.com/bxp151/exploretransform)

<br>
---

### Data Science Projects

- [(IN-PROGRESS) Predicting car prices](https://github.com/bxp151/usedcars) - The objective is to predict passenger vehicle prices using all Craigslist posts from October 23, 2020 to December 3, 2020. I am using NHTSA's VIN decoder API to capture verifiable information about each vehicle and used Google's Geocoding API to retrieve location information.  I have also built two text classifiers using DistilBERT and Google BERT to classify each sale as either private or dealer based on the description.
<img src="./images/usedcars.jpg" />

<br>
---

- [Predicting vehicle loan defaults](https://github.com/bxp151/ltfs) - Financial institutions have incurred significant losses due to vehicle loans defaults. This has led to the restriction of vehicle loan underwriting and increased vehicle loan rejection rates. As part of a hackathon, LTFS India has provided data that I used to predict vehicle loan defaults.
<img src="./images/cars.jpg" />

<br>
---


### Towards Data Science Articles

- [Are you dropping too many correlated features?](https://towardsdatascience.com/are-you-dropping-too-many-correlated-features-d1c96654abe6) - Some commonly used correlation filtering methods have a tendency to drop more features than required. This problem is amplified as datasets become larger and with more pairwise correlations above a specified threshold. If we drop more variables than necessary, less information will be available potentially leading to suboptimal model performance
<img src="https://miro.medium.com/max/700/0*EJw_Da7iRkwGh21N"/>

---

- [Accelerate your exploratory data analysis with exploretransform](https://towardsdatascience.com/make-exploratory-data-analysis-eda-faster-74c434595bcf) - 
Data scientists spend 60% of their time on cleaning and organizing data. This article demonstrates the exploretransform package which can help save time during exploratory data analysis. 
<img src="https://miro.medium.com/max/700/0*7EuVnbnoAu9yA4uU"/>

---

- [Maximum likelihood the easy way](https://towardsdatascience.com/maximum-likelihood-the-easy-way-1f14c0e2a5ce) - Demonstration of Maximum Likelihood Estimation (MLE) using a simple example, fitting a logistic regression model, and finally linking these concepts to GLMs.
<img src="https://miro.medium.com/max/700/0*YiRBPZGwtJQAxxNn"/>

---


- [Building machine learning intuition through play](https://towardsdatascience.com/building-machine-learning-intuition-through-play-2065fe487d46) - Using maximum likelihood to show why we minimize the sum of squared residuals in linear regression
<img src="https://miro.medium.com/max/700/0*uUyUNGhI43p4MRS8"/>

---
